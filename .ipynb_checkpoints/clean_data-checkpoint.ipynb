{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleans the data for a given dataset\n",
    "\n",
    "Note: To clean all the datasets (1-13), run run_clusters.ipynb\n",
    "      To clean just one dataset, uncomment the line below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning data\n"
     ]
    }
   ],
   "source": [
    "print('cleaning data')\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab random 200,000 entries from the original file, put them in a \n",
    "# new csv called 'data.csv'\n",
    "\n",
    "import random\n",
    "FILE = \"read_data/read_data_{}.csv\".format(file)\n",
    "data = pd.read_csv(FILE)\n",
    " \n",
    "try:\n",
    "    n =  len(data)\n",
    "    s = 200000 \n",
    "    skip = sorted(random.sample(range(1,n+1),n-s)) \n",
    "    data = pd.read_csv(FILE, skiprows=skip)\n",
    "    data.to_csv(\"working_data/data.csv\", index=False)\n",
    "except:\n",
    "    data.to_csv(\"working_data/data.csv\", index=False)\n",
    "    \n",
    "print('data retrieved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Drop missing entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing data\n",
    "\n",
    "data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert all the data to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Dummy Values for Proto, State, and Dir\n",
    "# ---------------\n",
    "\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(data['Proto'])\n",
    "data['Proto'] = labelEncoder.transform(data['Proto'])\n",
    "\n",
    "labelEncoder.fit(data['State'])\n",
    "data['State'] = labelEncoder.transform(data['State'])\n",
    "\n",
    "labelEncoder.fit(data['Dir'])\n",
    "data['Dir'] = labelEncoder.transform(data['Dir'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert StartTime to epoch\n",
    "# ---------------\n",
    "\n",
    "start_time = data['StartTime'].tolist()\n",
    "\n",
    "for index in range(0, len(start_time)):\n",
    "    start_time[index] = pd.to_datetime(start_time[index]).timestamp() \n",
    "    \n",
    "data['StartTime'] = start_time \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dst/src addresses to numerical values (delete the row if the value cannot be converted)\n",
    "# ----------------\n",
    "\n",
    "\n",
    "src_addr = data['SrcAddr'].tolist()\n",
    "dst_addr = data['DstAddr'].tolist()\n",
    "\n",
    "for index in range(0, len(src_addr)):\n",
    "    try:\n",
    "        src_addr[index] = int(str(src_addr[index]).replace('.', ''))\n",
    "    except: \n",
    "        data = data.drop(data.index[index])\n",
    "    \n",
    "for index in range(0, len(dst_addr)):\n",
    "    try: \n",
    "        dst_addr[index] = int(str(dst_addr[index]).replace('.', ''))\n",
    "    except: \n",
    "        data = data.drop(data.index[index])\n",
    "    \n",
    "data[\"SrcAddr\"] = src_addr\n",
    "data[\"DstAddr\"] = dst_addr\n",
    "data.to_csv(\"working_data/data.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sport and Dport to numerical values\n",
    "# ----------------\n",
    "\n",
    "rows = list(data['Sport'])\n",
    "for row_index in range(0, len(rows)):\n",
    "    try:\n",
    "        rows[row_index] = row[row_index].astype(np.float32)\n",
    "    except:\n",
    "        rows[row_index] = 0\n",
    "data['Sport'] = rows\n",
    "\n",
    "\n",
    "rows = list(data['Dport'])\n",
    "for row_index in range(0, len(rows)):\n",
    "    try:\n",
    "        rows[row_index] = rows[row_index].astype(np.float32)\n",
    "    except:\n",
    "        rows[row_index] = 0\n",
    "data['Dport'] = rows\n",
    "\n",
    "print('all but botnet converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Label column to numerical values. 1 if botnet, 0 if benign. \n",
    "# Store the botnet and benign flows in separate csv files.\n",
    "# -----------------------\n",
    "\n",
    "# grab all of the labels, store them in a list\n",
    "label_entries = list(data['Label'])\n",
    "\n",
    "# create lists for storing botnet and benign flows\n",
    "botnets = list()\n",
    "benign = list()\n",
    "\n",
    "# convert botnet entries to 1, benign to 0\n",
    "# -------------------\n",
    "index = 0\n",
    "for entry in data['Label']:\n",
    "    # non-TCP and UDP protocols are outliers, so remove them\n",
    "    if not 'TCP' in entry and not 'UDP' in entry:\n",
    "        label_entries[index] = -1\n",
    "    else:\n",
    "        if 'botnet' in entry or 'Botnet' in entry:\n",
    "            label_entries[index] = 1\n",
    "\n",
    "        else:\n",
    "            if not entry == 1 and not entry == 0:\n",
    "                label_entries[index] = 0\n",
    "    index += 1\n",
    "\n",
    "# Store the benign and botnet data in separate arrays\n",
    "# Convert benign and botnet to dataframes\n",
    "\n",
    "index = 0\n",
    "for entry in label_entries:\n",
    "    if entry == 1:\n",
    "        botnets.append(list(data.iloc[index]))\n",
    "    elif entry == 0:\n",
    "        benign.append(list(data.iloc[index]))\n",
    "    index += 1\n",
    "\n",
    "benign = DataFrame (benign,columns=list(data.columns))\n",
    "botnets = DataFrame (botnets,columns=list(data.columns))\n",
    "\n",
    "# Save the dataframes to csv files\n",
    "botnets.to_csv(\"working_data/botnet{}.csv\".format(file), index=False)\n",
    "benign.to_csv(\"working_data/benign.csv\", index=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "# ----------------\n",
    "\n",
    "# create lists for storing outliers and non-outliers\n",
    "no_outliers = list()\n",
    "outliers = list()\n",
    "\n",
    "# loop through each relevant column to find the outliers\n",
    "for name in ['Dur', 'TotBytes', 'SrcBytes', 'TotPkts']:\n",
    "    \n",
    "    # get the column from the list of non-malware\n",
    "    col = benign[name]\n",
    "    \n",
    "    # calculate summary statistics\n",
    "    data_mean, data_std = mean(col), std(col)\n",
    "    \n",
    "    # identify outliers\n",
    "    cut_off = data_std * 3\n",
    "    lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "\n",
    "    # find locations of the outliers\n",
    "    index = 0\n",
    "    for x in col:\n",
    "        #check for protocol outliers\n",
    "        if name == 'Label':\n",
    "            if x == -1:\n",
    "                outliers.append(index)\n",
    "                \n",
    "        if x < lower or x > upper:\n",
    "            outliers.append(index)\n",
    "        index += 1\n",
    "    \n",
    "#remove duplicates\n",
    "outliers = list(dict.fromkeys(outliers))   \n",
    "\n",
    "print('data cleaned')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save cleaned data to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the reduced list of benign flows with the botnet flows\n",
    "# Convert the list to csv\n",
    "\n",
    "no_outliers = DataFrame (pd.concat([benign.drop(benign.index[outliers]), botnets], ignore_index=True))\n",
    "\n",
    "# no_outliers.to_csv(\"working_data/no_outliers.csv\", index=False)\n",
    "no_outliers.to_csv(\"working_data/no_outliers{}.csv\".format(file), index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success!\n"
     ]
    }
   ],
   "source": [
    "print('success!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
